{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>This package provides a toolbox for LiDAR-Camera calibration that is: </p> <ul> <li>Generalizable: It can handle various LiDAR and camera projection models including spinning and non-repetitive scan LiDARs, and pinhole, fisheye, and omnidirectional projection cameras.</li> <li>Target-less: It does not require a calibration target but uses the environment structure and texture for calibration.</li> <li>Single-shot: At a minimum, only one pairing of a LiDAR point cloud and a camera image is required for calibration. Optionally, multiple LiDAR-camera data pairs can be used for improving the accuracy.</li> <li>Automatic: The calibration process is automatic and does not require an initial guess.</li> <li>Accurate and robust: It employs a pixel-level direct LiDAR-camera registration algorithm that is more robust and accurate compared to edge-based indirect LiDAR-camera registration.</li> </ul> <p> Video</p>"},{"location":"#getting-started","title":"Getting started","text":"<ol> <li>Installation / Docker images</li> <li>Data collection</li> <li>Calibration example</li> <li>Program details</li> </ol>"},{"location":"#license","title":"License","text":"<p>This package is released under the MIT license.</p>"},{"location":"#publication","title":"Publication","text":"<p>Koide et al., General, Single-shot, Target-less, and Automatic LiDAR-Camera Extrinsic Calibration Toolbox, ICRA2023, [PDF]</p>"},{"location":"#contact","title":"Contact","text":"<p>Kenji Koide, National Institute of Advanced Industrial Science and Technology (AIST), Japan</p>"},{"location":"collection/","title":"Data collection","text":""},{"location":"collection/#prerequisite","title":"Prerequisite","text":"<ul> <li>The intrinsic parameters of the camera (i.e., camera matrix and distortion coefficients) need to be calibrated beforehand.</li> <li>The camera and LiDAR must be rigidly fixed.</li> </ul>"},{"location":"collection/#data-collection-steps","title":"Data collection steps","text":"<ul> <li>Keep the sensor at rest and start recording <code>Image</code> and <code>PointCloud2</code> messages. We recommend recording <code>CameraInfo</code> as well if it is available.</li> <li>[For a non-repetitive scan LiDAR (Livox)] Wait for 10 ~ 15 sec without moving the sensor.</li> <li>[For a spinning LiDAR (Ouster)] Move the sensor up and down slowly for 10 sec. If your LiDAR has fewer scan lines (e.g., 16 or 32 lines), keep moving it a bit longer (e.g., 20 ~ 30 sec).</li> <li>Stop recording.</li> </ul> <p>Note</p> <p>While the calibration can be performed with only one rosbag at a minimum, we recommend taking several (5 ~ 10) rosbags for better calibration results.</p> <p>Data acquisition example for an Ouster OS1-128:</p>"},{"location":"collection/#keep-in-mind","title":"Keep in mind","text":"<ul> <li>The sensor must be kept at rest at the beginning of each rosbag to ensure that the first point cloud and image are taken at the same pose.</li> <li>LiDAR intensity data must exhibit some texture and geometry information. Be aware of that some LiDARs (e.g., Livox Avia) returns invalid intensity values for too close points.</li> </ul>"},{"location":"docker/","title":"Docker images","text":"<p>We provide ROS1/ROS2 docker images on our docker hub repository:</p> <ul> <li>koide3/direct_visual_lidar_calibration:noetic </li> <li>koide3/direct_visual_lidar_calibration:humble </li> <li>koide3/direct_visual_lidar_calibration:jazzy </li> </ul> <p>Warning</p> <p>We avoided including SuperGlue in our images to avoid contamination of its strict license. If you need the automatic image matching, you must build a docker image with Dockerfile_with_superglue at your own risk.</p>"},{"location":"docker/#pull-image","title":"Pull image","text":"<pre><code>docker pull koide3/direct_visual_lidar_calibration:humble\n</code></pre>"},{"location":"docker/#run-programs","title":"Run programs","text":"<p>Example1: Run preprocessing <pre><code>docker run \\\n--rm \\\n-v /path/to/input/bags:/tmp/input_bags \\\n-v /path/to/save/result:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:humble \\\nros2 run direct_visual_lidar_calibration preprocess -a /tmp/input_bags /tmp/preprocessed\n</code></pre></p> <p>Example2: Run preprocessing with GUI <pre><code>docker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v /path/to/input/bags:/tmp/input_bags \\\n-v /path/to/save/result:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:humble \\\nros2 run direct_visual_lidar_calibration preprocess -a -v /tmp/input_bags /tmp/preprocessed\n</code></pre></p>"},{"location":"docker/#examples","title":"Examples","text":"<p>See also Calibration example page.</p> Full commands for Livox-camera calibration example on docker (ROS2) <pre><code>bag_path=$(realpath livox)\npreprocessed_path=$(realpath livox_preprocessed)\n# Preprocessing\ndocker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $bag_path:/tmp/input_bags \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:humble \\\nros2 run direct_visual_lidar_calibration preprocess -av /tmp/input_bags /tmp/preprocessed\n\n# Initial guess\ndocker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:humble \\\nros2 run direct_visual_lidar_calibration initial_guess_manual /tmp/preprocessed\n\n# Fine registration\ndocker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:humble \\\nros2 run direct_visual_lidar_calibration calibrate /tmp/preprocessed\n\n# Result inspection\ndocker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:humble \\\nros2 run direct_visual_lidar_calibration viewer /tmp/preprocessed\n</code></pre> Full commands for Livox-camera calibration example on docker (ROS1) <pre><code>bag_path=$(realpath livox_ros1)\npreprocessed_path=$(realpath livox_ros1_preprocessed)\n# Preprocessing\ndocker run \\\n-it \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $bag_path:/tmp/input_bags \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:noetic \\\nrosrun direct_visual_lidar_calibration preprocess -av \\\n--camera_model plumb_bob \\\n--camera_intrinsic 1452.711762456289,1455.877531619469,1265.25895179213,1045.818593664107 \\\n--camera_distortion_coeffs -0.04203564850455424,0.0873170980751213,0.002386381727224478,0.005629700706305988,-0.04251149335870252 \\\n/tmp/input_bags /tmp/preprocessed\n\n# Initial guess\ndocker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:noetic \\\nrosrun direct_visual_lidar_calibration initial_guess_manual /tmp/preprocessed\n\n# Fine registration\ndocker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:noetic \\\nrosrun direct_visual_lidar_calibration calibrate /tmp/preprocessed\n\n# Result inspection\ndocker run \\\n--rm \\\n--net host \\\n--gpus all \\\n-e DISPLAY=$DISPLAY \\\n-v $HOME/.Xauthority:/root/.Xauthority \\\n-v $preprocessed_path:/tmp/preprocessed \\\nkoide3/direct_visual_lidar_calibration:noetic \\\nrosrun direct_visual_lidar_calibration viewer /tmp/preprocessed\n</code></pre>"},{"location":"example/","title":"Calibration example","text":""},{"location":"example/#dataset","title":"Dataset","text":"<p>LiDAR-camera calibration dataset (Zenodo)</p> <ul> <li>livox.tar.gz (ROS2)</li> <li>ouster.tar.gz (ROS2)</li> <li>livox_ros1.tar.gz (ROS1)</li> <li>ouster_ros1.tar.gz (ROS1)</li> </ul> <p>Note</p> <p>CameraInfo messages in the example ROS1 bag files exhibit an MD5 checksum that is different from that of the standard sensor_msgs/CameraInfo msg due to ROS2-ROS1 bag conversion issues. Because this affects the automatic camera info extraction, you need to manually specify camera parameters for preprocessing.</p>"},{"location":"example/#livox-camera-calibration","title":"Livox-camera calibration","text":""},{"location":"example/#1-download-dataset","title":"1. Download dataset","text":"<p>Download livox.tar.gz from zenodo repository and unzip it: <pre><code>$ tar xzvf livox.tar.gz\n</code></pre></p> <p>The livox dataset contains five ros2 bags, and each bag contains points/image/camera_info topics: <pre><code>$ ls livox\n# rosbag2_2023_03_09-13_42_46  rosbag2_2023_03_09-13_44_10  rosbag2_2023_03_09-13_44_54  rosbag2_2023_03_09-13_46_10  rosbag2_2023_03_09-13_46_54\n$ ros2 bag info livox/rosbag2_2023_03_09-13_42_46/\n# Files:             rosbag2_2023_03_09-13_42_46_0.db3\n# Bag size:          582.9 MiB\n# Storage id:        sqlite3\n# Duration:          15.650s\n# Start:             Mar  9 2023 13:42:46.665 (1678336966.665)\n# End:               Mar  9 2023 13:43:02.316 (1678336982.316)\n# Messages:          2972\n# Topic information: Topic: /livox/points | Type: sensor_msgs/msg/PointCloud2 | Count: 157 | Serialization Format: cdr\n#                    Topic: /livox/imu | Type: sensor_msgs/msg/Imu | Count: 2597 | Serialization Format: cdr\n#                    Topic: /livox/lidar | Type: livox_interfaces/msg/CustomMsg | Count: 157 | Serialization Format: cdr\n#                    Topic: /image | Type: sensor_msgs/msg/Image | Count: 30 | Serialization Format: cdr\n#                    Topic: /camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 31 | Serialization Format: cdr\n</code></pre></p>"},{"location":"example/#2-preprocessing","title":"2. Preprocessing","text":"<pre><code># -a : Detect points/image/camera_info topics automatically\n# -v : Enable visualization\n$ ros2 run direct_visual_lidar_calibration preprocess livox livox_preprocessed -av\n</code></pre> <p>After running <code>preprocess</code>, you can find a directory named <code>livox_preprocessed</code> that containts generated dense point clouds, camera images, and some meta data (screenshot):</p> <pre><code>$ ls livox_preprocessed/\n# calib.json                                         rosbag2_2023_03_09-13_44_10_lidar_intensities.png  rosbag2_2023_03_09-13_44_54.png                    rosbag2_2023_03_09-13_46_54_lidar_intensities.png\n# rosbag2_2023_03_09-13_42_46_lidar_indices.png      rosbag2_2023_03_09-13_44_10.ply                    rosbag2_2023_03_09-13_46_10_lidar_indices.png      rosbag2_2023_03_09-13_46_54.ply\n# rosbag2_2023_03_09-13_42_46_lidar_intensities.png  rosbag2_2023_03_09-13_44_10.png                    rosbag2_2023_03_09-13_46_10_lidar_intensities.png  rosbag2_2023_03_09-13_46_54.png\n# rosbag2_2023_03_09-13_42_46.ply                    rosbag2_2023_03_09-13_44_54_lidar_indices.png      rosbag2_2023_03_09-13_46_10.ply\n# rosbag2_2023_03_09-13_42_46.png                    rosbag2_2023_03_09-13_44_54_lidar_intensities.png  rosbag2_2023_03_09-13_46_10.png\n# rosbag2_2023_03_09-13_44_10_lidar_indices.png      rosbag2_2023_03_09-13_44_54.ply                    rosbag2_2023_03_09-13_46_54_lidar_indices.png\n</code></pre>"},{"location":"example/#3a-initial-guess-automatic","title":"3a. Initial guess (Automatic)","text":"<p>Warning</p> <p>SuperGlue is not allowed to be used for commercial purposes. If you are working for profit, use the manual initial guess estimation instead.</p> <p>Run SuperGlue to find correspondences between LiDAR can camera images. Because SuperGlue requires the upward directions of images are roughtly aligned, add <code>---rotate_camera 90</code> option to rotate camera images: <pre><code>$ ros2 run direct_visual_lidar_calibration find_matches_superglue.py livox_preprocessed --rotate_camera 90\n</code></pre></p> <p>Then, run <code>initial_guess_auto</code> to perform RANSAC-based initial guess estimation: <pre><code>$ ros2 run direct_visual_lidar_calibration initial_guess_auto livox_preprocessed\n</code></pre></p> <p></p>"},{"location":"example/#3b-initial-guess-manual","title":"3b. Initial guess (Manual)","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration initial_guess_manual livox_preprocessed\n</code></pre> <ol> <li>Right click a 3D point on the point cloud and a corresponding 2D point on the image</li> <li>Click <code>Add picked points</code> button</li> <li>Repeat 1 and 2 for several points (At least three points. The more the better.)</li> <li>Click <code>Estimate</code> button to obtain an initial guess of the LiDAR-camera transformation</li> <li>Check if the image projection result is fine by changing <code>blend_weight</code></li> <li>Click <code>Save</code> button to save the initial guess</li> </ol>"},{"location":"example/#4-fine-registration","title":"4. Fine registration","text":"<p>Perform NID-based fine LiDAR-camera registration to refine the LiDAR-camera transformation estimate: <pre><code>$ ros2 run direct_visual_lidar_calibration calibrate livox_preprocessed\n</code></pre></p>"},{"location":"example/#5-calibration-result-inspection","title":"5. Calibration result inspection","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration calibrate livox_preprocessed\n</code></pre>"},{"location":"example/#6-calibration-result-file","title":"6. Calibration result file","text":"<p>Once the calibration is completed, open <code>livox_preprocessed/calib.json</code> with a text editor and find the calibration result <code>T_lidar_camera: [x, y, z, qx, qy, qz, qw]</code> that transforms a 3D point in the camera frame into the LiDAR frame (i.e., <code>p_lidar = T_lidar_camera * p_camera</code>).</p> <p><code>calib.json</code> also contains camera parameters, manual/automatic initial guess results (<code>init_T_lidar_camera</code> and <code>init_T_lidar_camera_auto</code>), and some meta data.</p> calib.json example <pre><code>{\n\"camera\": {\n\"camera_model\": \"plumb_bob\",\n\"distortion_coeffs\": [\n-0.04203564850455424,\n0.0873170980751213,\n0.002386381727224478,\n0.005629700706305988,\n-0.04251149335870252\n],\n\"intrinsics\": [\n1452.711762456289,\n1455.877531619469,\n1265.25895179213,\n1045.818593664107\n]\n},\n\"meta\": {\n\"bag_names\": [\n\"rosbag2_2023_03_09-13_42_46\",\n\"rosbag2_2023_03_09-13_44_10\",\n\"rosbag2_2023_03_09-13_44_54\",\n\"rosbag2_2023_03_09-13_46_10\",\n\"rosbag2_2023_03_09-13_46_54\"\n],\n\"camera_info_topic\": \"/camera_info\",\n\"data_path\": \"livox\",\n\"image_topic\": \"/image\",\n\"intensity_channel\": \"intensity\",\n\"points_topic\": \"/livox/points\"\n},\n\"results\": {\n\"T_lidar_camera\": [\n0.023215513184544914,\n-0.049304803782681345,\n-0.0010268378243773314,\n0.002756788930227678,\n0.7121675520572427,\n0.0038417302647440915,\n0.7019936032615696\n],\n\"init_T_lidar_camera_auto\": [\n0.01329274206061581,\n-0.055999414521382934,\n0.0033183505131586903,\n0.002471267432195032,\n0.7121558216581672,\n0.0030750358632291534,\n0.7020103437059168\n]\n}\n}\n</code></pre> <p>Tip</p> <p>If you need the transformation in another form (e.g., 4x4 matrix), use the Matrix converter.</p>"},{"location":"example/#ouster-camera-calibration","title":"Ouster-camera calibration","text":"<p>Most of the commands below are the same as those for the Livox dataset except for the options for the preprocessing. See the Livox example for detailed explanation.</p>"},{"location":"example/#1-download-dataset_1","title":"1. Download dataset","text":"<p>Download ouster.tar.gz from zenodo repository and unzip it: <pre><code>$ tar xzvf ouster.tar.gz\n</code></pre></p> <p>The ouster dataset contains two ros2 bags, and each bag contains points/image/camera_info topics: <pre><code>$ ls ouster\n# rosbag2_2023_03_28-16_25_54  rosbag2_2023_03_28-16_26_51\n$ ros2 bag info ouster/rosbag2_2023_03_28-16_25_54/\n# Files:             rosbag2_2023_03_28-16_25_54_0.db3\n# Bag size:          1.2 GiB\n# Storage id:        sqlite3\n# Duration:          29.14s\n# Start:             Mar 28 2023 16:25:54.559 (1679988354.559)\n# End:               Mar 28 2023 16:26:23.573 (1679988383.573)\n# Messages:          407\n# Topic information: Topic: /camera_info | Type: sensor_msgs/msg/CameraInfo | Count: 58 | Serialization Format: cdr\n#                    Topic: /image | Type: sensor_msgs/msg/Image | Count: 58 | Serialization Format: cdr\n#                    Topic: /points | Type: sensor_msgs/msg/PointCloud2 | Count: 291 | Serialization Format: cdr\n</code></pre></p>"},{"location":"example/#2-preprocessing_1","title":"2. Preprocessing","text":"<p>For spinning-type LiDARs, enable dynamic points integration to generate dense point clouds while compensating for the sensor motion. <pre><code># -a : Detect points/image/camera_info topics automatically\n# -d : Use dynamic points integrator\n# -v : Enable visualization\n$ ros2 run direct_visual_lidar_calibration preprocess ouster ouster_preprocessed -adv\n</code></pre></p>"},{"location":"example/#3a-initial-guess-automatic_1","title":"3a. Initial guess (Automatic)","text":"<p>Warning</p> <p>SuperGlue is not allowed to be used for commercial purposes.</p> <pre><code>$ ros2 run direct_visual_lidar_calibration find_matches_superglue.py ouster_preprocessed\n$ ros2 run direct_visual_lidar_calibration initial_guess_auto ouster_preprocessed\n</code></pre>"},{"location":"example/#3b-initial-guess-manual_1","title":"3b. Initial guess (Manual)","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration initial_guess_manual ouster_preprocessed\n</code></pre>"},{"location":"example/#4-fine-registration_1","title":"4. Fine registration","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration calibrate ouster_preprocessed\n</code></pre>"},{"location":"example/#5-calibration-result-inspection_1","title":"5. Calibration result inspection","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration calibrate ouster_preprocessed\n</code></pre>"},{"location":"example_map/","title":"Camera-map registration","text":""},{"location":"example_map/#1-download-dataset","title":"1. Download dataset","text":"<p>Download faro.tar.gz from zenodo repository and unzip it: <pre><code>$ tar xzvf faro.tar.gz\n</code></pre></p> <pre><code>$ ls faro\n# camera_params.txt  image00.jpg  map.ply\ncat faro/camera_params.txt\n# camera_model: plumb_bob\n# intrinsic: 2242.45,2242.69,1579.98,1172.42\n# distortion: -0.0272569,0.00207913,-0.00411347,-0.00208982,-0.00291759\n</code></pre>"},{"location":"example_map/#2-preprocessing","title":"2. Preprocessing","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration preprocess_map \\\n--map_path faro/map.ply \\\n--image_path faro/image00.jpg \\\n--dst_path faro_preprocessed \\\n--camera_model plumb_bob \\\n--camera_intrinsics 2242.45,2242.69,1579.98,1172.42 \\\n--camera_distortion_coeffs -0.0272569,0.00207913,-0.00411347,-0.00208982,-0.00291759\n</code></pre>"},{"location":"example_map/#3-initial-guess","title":"3. Initial guess","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration initial_guess_manual faro_preprocessed\n</code></pre>"},{"location":"example_map/#4-fine-registration","title":"4. Fine registration","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration calibrate faro_preprocessed\n</code></pre>"},{"location":"installation/","title":"Installation","text":"<p>Note</p> <p>We provide docker images so that the user can do calibration without installation: Docker images</p>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<ul> <li>ROS1/ROS2</li> <li>PCL</li> <li>OpenCV</li> <li>GTSAM</li> <li>Ceres</li> <li>Iridescence</li> <li>SuperGlue [optional]</li> </ul>"},{"location":"installation/#install-common-dependencies","title":"Install Common dependencies","text":"<pre><code># Install dependencies\nsudo apt install libomp-dev libboost-all-dev libglm-dev libglfw3-dev libpng-dev libjpeg-dev\n\n# Install GTSAM\ngit clone https://github.com/borglab/gtsam\ncd gtsam &amp;&amp; git checkout 4.2a9\nmkdir build &amp;&amp; cd build\n# For Ubuntu 22.04, add -DGTSAM_USE_SYSTEM_EIGEN=ON\ncmake .. -DGTSAM_BUILD_EXAMPLES_ALWAYS=OFF \\\n-DGTSAM_BUILD_TESTS=OFF \\\n-DGTSAM_WITH_TBB=OFF \\\n-DGTSAM_BUILD_WITH_MARCH_NATIVE=OFF\nmake -j$(nproc)\nsudo make install\n\n# Install Ceres\ngit clone https://github.com/ceres-solver/ceres-solver\nmkdir ceres-solver/build &amp;&amp; cd ceres-solver/build\ncmake .. -DBUILD_EXAMPLES=OFF -DBUILD_TESTING=OFF -DUSE_CUDA=OFF\nmake -j$(nproc)\nsudo make install\n\n# Install Iridescence for visualization\ngit clone https://github.com/koide3/iridescence --recursive\nmkdir iridescence/build &amp;&amp; cd iridescence/build\ncmake .. -DCMAKE_BUILD_TYPE=Release\nmake -j$(nproc)\nsudo make install\n</code></pre>"},{"location":"installation/#install-superglue-optional","title":"Install SuperGlue (Optional)","text":"<p>Warning</p> <p>SuperGlue is not allowed to be used for commercial purposes. You must carefully check and follow its licensing conditions.</p> <pre><code>pip3 install numpy opencv-python torch matplotlib\ngit clone https://github.com/magicleap/SuperGluePretrainedNetwork.git\n\necho 'export PYTHONPATH=$PYTHONPATH:/path/to/SuperGluePretrainedNetwork' &gt;&gt; ~/.bashrc\nsource ~/.bashrc\n</code></pre>"},{"location":"installation/#build-direct_visual_lidar_calibration","title":"Build direct_visual_lidar_calibration","text":"<pre><code># ROS1\ncd ~/catkin_ws/src\ngit clone https://github.com/koide3/direct_visual_lidar_calibration.git --recursive\ncd .. &amp;&amp; catkin_make\n</code></pre> <pre><code># ROS2\ncd ~/ros2_ws/src\ngit clone https://github.com/koide3/direct_visual_lidar_calibration.git --recursive\ncd .. &amp;&amp; colcon build\n</code></pre>"},{"location":"programs/","title":"Program details","text":""},{"location":"programs/#1-preprocessing","title":"1. Preprocessing","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration preprocess data_path dst_path\n</code></pre> Option Description -v [ --visualize ] Show points accumulation process -a [ --auto_topic ] Select image/camera_info/points topics automatically -d [ --dynamic_lidar_integration ] Create target point clouds from dynamic LiDAR data (for velodyne-like LiDARs) -i [ --intensity_channel ] \"auto\" or point intensity channel name --camera_info_topic CameraInfo topic name --image_topic Image topic name --points_topic PointCloud2 topic name --camera_model auto, atan, plumb_bob, fisheye, omnidir, or equirectangular --camera_intrinsic Camera intrinsic parameters: fx,fy,cx,cy(,xi) (Don't put spaces between values) --camera_distortion_coeffs Camera distortion parameters [k1,k2,p1,p2,k3] (Don't put spaces between values) --voxel_resolution Downsampling resolution --min_distance Minimum point distance. Points closer than this value will be discarded <p>Example1: Automatically select image/camera_info/points topics and use the dynamic point integrator for an Ouster LiDAR <pre><code># -a = Automatic topic selection\n# -d = Dynamic points integration\n# -v = Visualization\n$ ros2 run direct_visual_lidar_calibration preprocess ouster_pinhole ouster_pinhole_preprocessed -a -d -v\n</code></pre></p> <p>Note</p> <p>Automatic topic selection requires each rosbag to contain single image/camera_info/points topics.</p> <p>Note</p> <p><code>distortion_model</code> of the camera_info msg must be <code>plumb_bob</code> or <code>fisheye</code>. For other distortion models, specify the camera model name and parameters manually as in Example2.</p> <p>Example2: Manually specify image/points topics and camera parameters <pre><code>$ ros2 run direct_visual_lidar_calibration preprocess ouster_pinhole ouster_pinhole_preprocessed \\\n  --image_topic /camera/image \\\n  --points_topic /os_cloud_node/points \\\n  --camera_model plumb_bob \\\n  --camera_intrinsics 810.38,810.28,822.84,622.47\n  --camera_distortion_coeffs\n</code></pre></p>"},{"location":"programs/#2-initial-guess","title":"2. Initial guess","text":""},{"location":"programs/#option1-manual-estimation","title":"Option1: Manual estimation","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration initial_guess_manual preprocessed_data_path\n</code></pre> <ol> <li>Right click a 3D point on the point cloud and a corresponding 2D point on the image</li> <li>Click <code>Add picked points</code> button</li> <li>Repeat 1 and 2 for several points (At least three points. The more the better.)</li> <li>Click <code>Estimate</code> button to obtain an initial guess of the LiDAR-camera transformation</li> <li>Check if the image projection result is fine by changing <code>blend_weight</code></li> <li>Click <code>Save</code> button to save the initial guess</li> </ol>"},{"location":"programs/#option2-automatic-estimation","title":"Option2: Automatic estimation","text":"<p>Warning</p> <p>SuperGlue is not allowed to be used for commercial purposes!!</p> <pre><code>$ ros2 run direct_visual_lidar_calibration find_matches_superglue.py preprocessed_data_path\n$ ros2 run direct_visual_lidar_calibration initial_guess_auto preprocessed_data_path\n</code></pre> <p>Note</p> <p>SuperGlue requires the upward directions of camera and LiDAR images are roughly aligned. Use <code>--rotate_camera</code> and <code>--rotate_lidar</code> options to ensure this. (e.g., <code>--rotate_camera 90</code>. Only 90, 180, and 270 degree rotations are allowed.)</p> <p>Note</p> <p>You can find <code>*_superglue.png</code> saved in the data directory that represents detected 2D-3D correspondences.</p> <p></p>"},{"location":"programs/#3-calibration","title":"3. Calibration","text":"<pre><code>$ ros2 run direct_visual_lidar_calibration calibrate preprocessed_data_path\n</code></pre> Option Description --registration_type (default = nid_bfgs) <code>nid_bfgs</code> or <code>nid_nelder_mead</code> --nid_bins (default = 16) Number of histogram bins for NID --nelder_mead_init_step (default = 0.001) Nelder-mead initial step size --nelder_mead_convergence_criteria (default = 1e-8) Nelder-mead convergence criteria --auto_quit Automatically close the viewer after calibration --background Disable visualization <p>Once the calibration is completed, open <code>calib.json</code> in the data directory with a text editor and find the calibration result <code>T_lidar_camera: [x, y, z, qx, qy, qz, qw]</code> that transforms a 3D point in the camera frame into the LiDAR frame (i.e., <code>p_lidar = T_lidar_camera * p_camera</code>).</p> <p><code>calib.json</code> also contains camera parameters, manual/automatic initial guess results (<code>init_T_lidar_camera</code> and <code>init_T_lidar_camera_auto</code>), and some meta data.</p> calib.json example <pre><code>{\n\"camera\": {\n\"camera_model\": \"plumb_bob\",\n\"distortion_coeffs\": [\n-0.0408800300227048,\n0.08232065129613146,\n0.0001524417339184569,\n-0.0002905086459989649,\n-0.03955344846871078\n],\n\"intrinsics\": [\n810.3829359698531,\n810.2790141092258,\n822.8441591172331,\n622.4745298743934\n]\n},\n\"meta\": {\n\"bag_names\": [\n\"rosbag2_2022_12_09-11_51_00\",\n\"rosbag2_2022_12_09-11_51_39\",\n\"rosbag2_2022_12_09-11_52_13\",\n\"rosbag2_2022_12_09-11_52_50\",\n\"rosbag2_2022_12_09-11_53_36\"\n],\n\"camera_info_topic\": \"/drone01/camera_info\",\n\"data_path\": \"ouster_pinhole\",\n\"image_topic\": \"/drone01/image/compressed\",\n\"intensity_channel\": \"reflectivity\",\n\"points_topic\": \"/drone01/points\"\n},\n\"results\": {\n\"T_lidar_camera\": [\n0.029965406350829532,\n0.0018510163746144406,\n0.10836834957603067,\n-0.5020970411416648,\n0.49250979122625377,\n-0.5009468032383634,\n0.5043659060130069\n],\n\"init_T_lidar_camera\": [\n0.22754979133605957,\n0.14180368185043335,\n0.09482517838478088,\n-0.4999842630484326,\n0.4931746122747289,\n-0.5037605589800136,\n0.5030107918869041\n],\n\"init_T_lidar_camera_auto\": [\n-0.06012828990854561,\n0.03957544424313349,\n0.09638527740996672,\n-0.5015219498041896,\n0.4932277405168562,\n-0.5021955602061449,\n0.5029927923524125\n]\n}\n}\n</code></pre> <p>Tip</p> <p>If you need the transformation in another form (e.g., 4x4 matrix), use the Matrix converter.</p>"},{"location":"programs/#4-result-inspection","title":"4. Result inspection","text":"<p><pre><code>$ ros2 run direct_visual_lidar_calibration viewer preprocessed_data_path\n</code></pre> </p>"}]}